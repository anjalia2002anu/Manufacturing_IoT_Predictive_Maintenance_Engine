{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9s-JjDdU7LB"
      },
      "source": [
        "## **Task 1: Feature Engineering**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ovmNw5-e-En"
      },
      "source": [
        "##### **Load & Prepare the Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVfRMsdOTpy4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from joblib import dump"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "AiID5jo4g-QF"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyBLeqnnh1C1",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.listdir('/content')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcCA0OrCf7nF"
      },
      "outputs": [],
      "source": [
        "# Load training data\n",
        "df = pd.read_csv(\n",
        "    \"train_FD001.txt\",\n",
        "    sep=r\"\\s+\",\n",
        "    header=None\n",
        ") # Read the file and split columns wherever there are one or more spaces or tabs, and assume there is no header row."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXxdtb54gg7S"
      },
      "outputs": [],
      "source": [
        "# Column names based on NASA documentation\n",
        "cols = (\n",
        "    [\"engine_id\", \"cycle\"] +\n",
        "    [f\"op_setting_{i}\" for i in range(1, 4)] +\n",
        "    [f\"sensor_{i}\" for i in range(1, 22)]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNjKFCHNgkMe"
      },
      "outputs": [],
      "source": [
        "df.columns = cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftC2YFPzh8M_"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1zhF0taiTxP"
      },
      "source": [
        "##### **Create Remaining Useful Life (RUL)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqxeUR1HiC35"
      },
      "outputs": [],
      "source": [
        "# Max cycle per engine\n",
        "max_cycle = df.groupby(\"engine_id\")[\"cycle\"].max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgvo4BsWifq7"
      },
      "outputs": [],
      "source": [
        "# Map max cycle\n",
        "df[\"max_cycle\"] = df[\"engine_id\"].map(max_cycle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h60Aq4scik2K"
      },
      "outputs": [],
      "source": [
        "# Remaining Useful Life\n",
        "df[\"RUL\"] = df[\"max_cycle\"] - df[\"cycle\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tkjh5Hcit0R"
      },
      "source": [
        "##### **Create 24-Hour Failure Label (Classification Target)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8XI9wxZio0h"
      },
      "outputs": [],
      "source": [
        "FAILURE_WINDOW = 24  # 24 hours / cycles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nASbnRpwizCw"
      },
      "outputs": [],
      "source": [
        "df[\"failure_next_24hrs\"] = (df[\"RUL\"] <= FAILURE_WINDOW).astype(int) # If Remaining Useful Life (RUL) â‰¤ 24 cycles, failure will happen within the next 24 hours"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9f0HUfoi4xH"
      },
      "outputs": [],
      "source": [
        "df[['RUL','failure_next_24hrs']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5z0J8NOMjFc4"
      },
      "source": [
        "#### **Feature Engineering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ug5-o3P6jVOu"
      },
      "source": [
        "##### **A. Rolling Mean & Standard Deviation(Last 1, 6, 12 hours)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Av1YFu9i6yf"
      },
      "outputs": [],
      "source": [
        "WINDOWS = [6, 12] # rolling time windows\n",
        "SENSORS = [f\"sensor_{i}\" for i in range(1, 22)] # list of sensor columns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why 1 is NOT included in WINDOWS = [6, 12]?\n",
        "* The rolling mean of 1 value = the value itself\n",
        "* So this feature is identical to the original sensor\n",
        "* It adds no new pattern or trend\n",
        "* Standard deviation needs at least 2 values\n",
        "* With 1 value â†’ result is NaN"
      ],
      "metadata": {
        "id": "0iHV78y3te2Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdc3QpTIiIWg"
      },
      "outputs": [],
      "source": [
        "feature_dict = {} # Collect features in a dictionary which helps to generate features WITHOUT inserting into df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "tcxVedLgjjKn"
      },
      "outputs": [],
      "source": [
        "for sensor in SENSORS:\n",
        "    for w in WINDOWS:\n",
        "        feature_dict[f\"{sensor}_roll_mean_{w}\"] = (\n",
        "            df.groupby(\"engine_id\")[sensor] # Each engine has its own independent life cycle. Rolling stats are calculated only within the same engine\n",
        "              .rolling(window=w) # Look at the previous w time steps, including the current one.\n",
        "              .mean() # it captures: Overall trend & Smooths noisy sensor data\n",
        "              .reset_index(level=0, drop=True)# groupby().rolling() creates a MultiIndex. Pandas canâ€™t assign it directly to the DataFrame .Removes the engine_id index level & Aligns values correctly with original rows\n",
        "        )\n",
        "\n",
        "        feature_dict[f\"{sensor}_roll_std_{w}\"] = (\n",
        "            df.groupby(\"engine_id\")[sensor]\n",
        "              .rolling(window=w)\n",
        "              .std() # it captures: Variability / instability & Sudden fluctuations often indicate degradation\n",
        "              .reset_index(level=0, drop=True)\n",
        "        )\n",
        "# For every sensor and every time window, it creates rolling statistical features (mean and standard deviation) separately for each engine.\n",
        "# This helps the ML model understand trends and variability in sensor behavior over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAMemCIXirkW"
      },
      "outputs": [],
      "source": [
        "rolling_features = pd.DataFrame(feature_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12TZ_rO4iy6P"
      },
      "outputs": [],
      "source": [
        "df = pd.concat([df, rolling_features], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZDDro6Wj7GI"
      },
      "source": [
        "### **B. Exponential Moving Average (EMA)**\n",
        "\n",
        "EMA gives more weight to recent sensor values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQZPKiGujeIk"
      },
      "outputs": [],
      "source": [
        "ema_features = {} # It helps to stores the generated EMA feature in a dictionary instead of directly adding it to df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjrxB1aWjwxn"
      },
      "outputs": [],
      "source": [
        "for sensor in SENSORS: # Loops through all sensor columns\n",
        "    ema_features[f\"{sensor}_ema_6\"] = (\n",
        "        df.groupby(\"engine_id\")[sensor] # Groups data by engine. Ensures EMA is calculated per engine lifecycle\n",
        "          .ewm(span=6, adjust=False) # Applies Exponential Weighted Moving Average. span=6 â†’ recent 6 time steps get higher weight. adjust=False â†’ uses recursive EMA formula (standard in ML & signal processing)\n",
        "          .mean() # Computes the EMA values. Despite the name, EMA is not a simple averageâ€”it weights recent values more heavily.\n",
        "          .reset_index(level=0, drop=True) # Removes the engine_id index created by groupby()\n",
        "    )\n",
        "\n",
        "    ema_features[f\"{sensor}_ema_12\"] = (\n",
        "        df.groupby(\"engine_id\")[sensor]\n",
        "          .ewm(span=12, adjust=False)\n",
        "          .mean()\n",
        "          .reset_index(level=0, drop=True)\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code creates Exponential Moving Average (EMA) features for each sensor, calculated separately for each engine, using two time windows:\n",
        "\n",
        "* EMA(6) â†’ short-term behavior\n",
        "\n",
        "* EMA(12) â†’ medium-term behavior\n",
        "\n",
        "These features help the model detect early degradation patterns in sensor readings."
      ],
      "metadata": {
        "id": "2LX-JAYhuU5F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRF2rS0Qjvjj"
      },
      "outputs": [],
      "source": [
        "ema_df = pd.DataFrame(ema_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0Y3nvCPjzGP"
      },
      "outputs": [],
      "source": [
        "df = pd.concat([df, ema_df], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Jdd5XGkkLQf"
      },
      "source": [
        "### **C. Lag Features (t-1, t-2)**\n",
        "\n",
        "Lag features capture temporal dependency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvBs0tVjoKCS"
      },
      "source": [
        "This creates lag (historical) features for each sensor so the ML model can learn:\n",
        "\n",
        "How past sensor values influence future failures\n",
        "\n",
        "Lag features are essential in time-series prediction and prevent data leakage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hm1DPnxVkFXE"
      },
      "outputs": [],
      "source": [
        "LAGS = [1, 2] #Lag-1 â†’ previous time step\n",
        "# Lag-2 â†’ two time steps ago"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7rG3robnApf"
      },
      "outputs": [],
      "source": [
        "lag_features = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZX923ZtkS8t"
      },
      "outputs": [],
      "source": [
        "for sensor in SENSORS:\n",
        "    for lag in LAGS:\n",
        "        lag_features[f\"{sensor}_lag_{lag}\"] = (\n",
        "            df.groupby(\"engine_id\")[sensor]\n",
        "              .shift(lag) # Shifts sensor values backward in time & Creates historical context\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNJGsFxAnLbt"
      },
      "outputs": [],
      "source": [
        "lag_df = pd.DataFrame(lag_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mtGqwrNnPH5"
      },
      "outputs": [],
      "source": [
        "df = pd.concat([df, lag_df], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFJLsRPtnQN6"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xxjza8r-kf_S"
      },
      "source": [
        "### **Handle Missing Values**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rolling and lag features create NaN values at the beginning of each time series.\n",
        "\n",
        "The maximum window or lag tells us how many initial rows must be dropped to remove all NaNs safely."
      ],
      "metadata": {
        "id": "AtMTkCLKIB_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_WINDOW = max(max(WINDOWS), max(LAGS)) #finds the largest value among all rolling window sizes (WINDOWS) and lag steps (LAGS)"
      ],
      "metadata": {
        "id": "lAohKCTI90zQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_WINDOW"
      ],
      "metadata": {
        "id": "ppFLVnntu_CX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[df.groupby(\"engine_id\").cumcount() >= MAX_WINDOW].reset_index(drop=True)\n",
        "#removes the first few rows of each engine where time-series features (rolling, lag, EMA) are not fully available.\n",
        "#cumcount():It counts rows within each group, starting from 0, and increases by 1 for every new row in that group."
      ],
      "metadata": {
        "id": "6o4C9dWh_uHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFA35t-Hkp2D"
      },
      "source": [
        "### **Select Final Feature Set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDoSoC56kmcz"
      },
      "outputs": [],
      "source": [
        "FEATURE_COLUMNS = [\n",
        "    col for col in df.columns\n",
        "    if \"sensor_\" in col and\n",
        "    (\"roll\" in col or \"ema\" in col or \"lag\" in col)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4auJpRkJk0Xd"
      },
      "outputs": [],
      "source": [
        "X = df[FEATURE_COLUMNS]\n",
        "y = df[\"failure_next_24hrs\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.isna().sum().sum()"
      ],
      "metadata": {
        "id": "IYPUdw-w8jmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tnYrUFSk7q6"
      },
      "source": [
        "### **Efficient Serialization Using joblib**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlM4jbnqk3Ri"
      },
      "outputs": [],
      "source": [
        "dump(X, \"X_features_FD001.joblib\")# dump() serializes (converts) Python objects into binary files. Files are saved on disk with .joblib extension\n",
        "dump(y, \"y_labels_FD001.joblib\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MFBMuDPlKxL"
      },
      "outputs": [],
      "source": [
        "from joblib import load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBkhRJERmZCe"
      },
      "outputs": [],
      "source": [
        "X = load(\"X_features_FD001.joblib\") # Reads binary .joblib files. Reconstructs the original Python objects in memory\n",
        "y = load(\"y_labels_FD001.joblib\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Task 2: Modeling**\n"
      ],
      "metadata": {
        "id": "F41ahPpmza9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "UTBC2tUS3ZkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42,stratify=y)\n",
        "#stratify=y: Ensures failures appear in both train & test. Mandatory for <1% imbalance"
      ],
      "metadata": {
        "id": "dd4OSNuY3bOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.head()"
      ],
      "metadata": {
        "id": "YhZAuVJMzFvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Baseline Model: Logistic Regression**\n"
      ],
      "metadata": {
        "id": "6HZaRlBA0b8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.pipeline import Pipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "hSpKd2qA68Kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logreg_pipeline = Pipeline(steps=[\n",
        "    (\"scaler\", StandardScaler()), #Scales all features to the same range so Logistic Regression works properly\n",
        "\n",
        "    (\"smote\", SMOTE(\n",
        "        sampling_strategy=\"auto\", #Generates synthetic samples for the minority class to balance the dataset.(applied only on training data, no data leakage).\n",
        "        random_state=42\n",
        "    )),\n",
        "\n",
        "    (\"model\", LogisticRegression(\n",
        "        max_iter=1000, #ensures convergence\n",
        "        class_weight=None, #not needed because SMOTE balances classes\n",
        "        n_jobs=-1 #uses all CPU cores for faster training\n",
        "    ))\n",
        "])"
      ],
      "metadata": {
        "id": "5yIGE8T5y_JD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logreg_pipeline.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "6hDILrDj6yXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate Logistic Regression (PR-AUC)\n",
        "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
        "\n",
        "y_probs_lr = logreg_pipeline.predict_proba(X_test)[:, 1] #Selects only the probability of failure (class = 1)\n",
        "pr_auc_lr = average_precision_score(y_test, y_probs_lr)\n",
        "#calculates the probability of positive class predictions and evaluates the model using precision-recall performance.\n",
        "\n",
        "print(\"Logistic Regression PR-AUC:\", pr_auc_lr)"
      ],
      "metadata": {
        "id": "6TPZdwZ67UGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Production Model: XGBoost**"
      ],
      "metadata": {
        "id": "t1V2ZufkEUbE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### **Hyperparameter Tuning via GridSearchCV**"
      ],
      "metadata": {
        "id": "Ym4Bfm4Nl9fb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "lCEGQF66EyG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count class distribution\n",
        "negative_samples = (y_train == 0).sum()\n",
        "positive_samples = (y_train == 1).sum() #Counts how many non-failure (0) and failure (1) samples are in the training data.\n",
        "\n",
        "scale_pos_weight = negative_samples / positive_samples #Computes how much more weight the model should give to the rare failure class\n",
        "\n",
        "print(\"scale_pos_weight:\", scale_pos_weight)"
      ],
      "metadata": {
        "id": "OzTMX_2a3wZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from sklearn.metrics import make_scorer, average_precision_score"
      ],
      "metadata": {
        "id": "xk5K-5nnntYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Pipeline\n",
        "xgb_pipeline = Pipeline(steps=[\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"xgb\", XGBClassifier(\n",
        "        objective=\"binary:logistic\",\n",
        "        eval_metric=\"aucpr\",\n",
        "        scale_pos_weight=scale_pos_weight,\n",
        "        n_jobs=-1,\n",
        "        random_state=42\n",
        "    ))\n",
        "])"
      ],
      "metadata": {
        "id": "pUc7hfBgqGkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter Grid\n",
        "param_grid = {\n",
        "    \"xgb__n_estimators\": [200, 400],\n",
        "    \"xgb__max_depth\": [4, 6],\n",
        "    \"xgb__learning_rate\": [0.05, 0.1],\n",
        "    \"xgb__subsample\": [0.8],\n",
        "    \"xgb__colsample_bytree\": [0.8],\n",
        "    \"xgb__min_child_weight\": [1, 5],\n",
        "    \"xgb__gamma\": [0, 1]\n",
        "}"
      ],
      "metadata": {
        "id": "i9DxE2RXfzLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GridSearchCV Setup\n",
        "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42) #Splits data into 3 folds.Stratified â†’ keeps class imbalance ratio the same in each fold. shuffle=True â†’ randomizes samples\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=xgb_pipeline,\n",
        "    param_grid=param_grid, #Tries all parameter combinations in param_grid\n",
        "    scoring=\"average_precision\",\n",
        "    cv=cv, #Uses stratified cross-validation\n",
        "    n_jobs=-1,\n",
        "    verbose=2,\n",
        "    error_score=\"raise\"  # helps debugging\n",
        ")"
      ],
      "metadata": {
        "id": "6DMVyT20q7rI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_search.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "JKV9EydKrMsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Best Parameters\n",
        "print(\"Best Parameters:\")\n",
        "print(grid_search.best_params_)"
      ],
      "metadata": {
        "id": "Q9UciFe0rRC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Final Model (GridSearch Best)\n",
        "best_xgb_grid = grid_search.best_estimator_"
      ],
      "metadata": {
        "id": "wNeEIOXVra3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate on Test Data\n",
        "y_pred_proba = best_xgb_grid.predict_proba(X_test)[:, 1]\n",
        "\n",
        "pr_auc = average_precision_score(y_test, y_pred_proba)\n",
        "print(\"Test PR-AUC:\", pr_auc)"
      ],
      "metadata": {
        "id": "9obfKC5drjCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Precisionâ€“Recall Curve"
      ],
      "metadata": {
        "id": "rYvLf1AWFfl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "qwGZyB-FFj4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(recall, precision, label=f\"PR-AUC = {pr_auc:.4f}\")\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision-Recall Curve\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Kq2RLZiFFZsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Precisionâ€“Recall curve shows excellent model performance with a PR-AUC of 0.9943. Precision remains close to 1.0 across most recall levels, indicating that the model accurately detects failures while minimizing false alarms."
      ],
      "metadata": {
        "id": "Myh0QuVb-Cqi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Task 3: Explainability Using SHAP**"
      ],
      "metadata": {
        "id": "m8LIcXGsYS2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shap"
      ],
      "metadata": {
        "id": "man28NDQuA45",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap"
      ],
      "metadata": {
        "id": "b0CXbBVMZIDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract Trained XGBoost Model\n",
        "xgb_model = best_xgb_grid.named_steps[\"xgb\"] #SHAP needs the actual trained model, not the full pipeline."
      ],
      "metadata": {
        "id": "x0kLxQiOZNAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create SHAP Explainer\n",
        "explainer = shap.TreeExplainer(xgb_model)"
      ],
      "metadata": {
        "id": "6rC1fyGmZc0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Compute SHAP Values\n",
        "X_sample = X_test.sample(200, random_state=42)\n",
        "\n",
        "shap_values = explainer.shap_values(X_sample)"
      ],
      "metadata": {
        "id": "WP89x5yHZoaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Local Explainability (Single Engine Prediction)**"
      ],
      "metadata": {
        "id": "Q10y6qX8ehnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "engine_index = 10  # choose any index\n",
        "shap.force_plot( #Creates a force plot\n",
        "    explainer.expected_value, #This is the average prediction of the model\n",
        "    shap_values[engine_index], #SHAP values tell how much each feature contributed\n",
        "    X_sample.iloc[engine_index], #Provides the actual sensor readings/features for engine 10\n",
        "    matplotlib=True\n",
        ")"
      ],
      "metadata": {
        "id": "6_bB2bR8cJh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Base Value**\n",
        "\n",
        "* The base value (shown near 0) is the average prediction of the model across all engines.\n",
        "\n",
        "* If no feature information was given, the model would predict this value.\n",
        "\n",
        "**Final Prediction (f(x) = 7.80)**\n",
        "\n",
        "* After considering all features, the model ends at 7.80.\n",
        "\n",
        "* This is the final output for this engine.\n",
        "\n",
        "| Color       | Meaning                          |\n",
        "| ----------- | -------------------------------- |\n",
        "| **Red**  | Feature increases the prediction |\n",
        "| **Blue** | Feature decreases the prediction |\n",
        "\n",
        "**sensor_9_roll_std_12 = 4.44**\n",
        "\n",
        "* This is the strongest contributor increasing the prediction\n",
        "\n",
        "* It measures rolling standard deviation of sensor 9 over a window of 12\n",
        "\n",
        "* High rolling standard deviation means:\n",
        "\n",
        "   Sensor values are highly unstable\n",
        "\n",
        "  Indicates abnormal operating conditions\n",
        "\n",
        "  Because of this instability, the model pushes the prediction upward (towards failure risk)\n",
        "\n",
        "ðŸ“Œ Interpretation:\n",
        "\n",
        "High fluctuation in sensor 9 strongly signals possible failure.\n",
        "\n",
        "**sensor_14_roll_std_12 = 3.05**\n",
        "\n",
        "* Also increases the prediction\n",
        "\n",
        "* Indicates variability in sensor 14 readings\n",
        "\n",
        "* Less impact than sensor 9, but still significant\n",
        "\n",
        "ðŸ“Œ Interpretation:\n",
        "\n",
        "Moderate instability in sensor 14 adds to the failure risk.\n",
        "\n",
        "**Blue features (right side)**\n",
        "\n",
        "* These features pull the prediction down\n",
        "\n",
        "* They represent:\n",
        "\n",
        "    Stable sensors\n",
        "\n",
        "    Normal operating conditions\n",
        "\n",
        "    However, their combined effect is weaker than the red features\n"
      ],
      "metadata": {
        "id": "MgtMHPRCGCbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Save SHAP Values\n",
        "dump(shap_values, \"shap_values.pkl\")\n",
        "dump(X_sample, \"shap_sample_data.pkl\")"
      ],
      "metadata": {
        "id": "YdIn6nc3e2hS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Task 4: Deployment**"
      ],
      "metadata": {
        "id": "MWmUiwheSpuQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Save the Trained Model\n",
        "import joblib\n",
        "\n",
        "joblib.dump(\n",
        "    best_xgb_grid,\n",
        "    \"/content/predictive_maintenance_model.pkl\"\n",
        ")\n",
        "\n",
        "print(\"Saved!\")\n",
        "print(os.listdir(\"/content\"))"
      ],
      "metadata": {
        "id": "DrY7tjS8UuG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "sample_input = X_train.iloc[0].to_dict()\n",
        "\n",
        "with open(\"sample_input.json\", \"w\") as f:\n",
        "    json.dump(sample_input, f)"
      ],
      "metadata": {
        "id": "3wsKQqc6WrTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"sample_input.json\")"
      ],
      "metadata": {
        "id": "VfHavh_jWxve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Build Flask API**"
      ],
      "metadata": {
        "id": "751QAKSi0MYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-cEzMkHa0EoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "from flask import Flask, request, jsonify\n",
        "import time\n",
        "import joblib"
      ],
      "metadata": {
        "id": "wTtl5wFw0VMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok.set_auth_token(\"37s4tpKvpHYzZDTZaR5fojyXlc4_48vCbozcVRERUDWzfLHtW\")"
      ],
      "metadata": {
        "id": "7OxB-_lD4Dj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model\n",
        "model = joblib.load(\"/content/predictive_maintenance_model.pkl\")\n",
        "print(\"Model loaded successfully\")"
      ],
      "metadata": {
        "id": "OyftNLnZWWcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save feature list ONCE\n",
        "feature_columns = list(X_train.columns)\n",
        "joblib.dump(feature_columns, \"feature_columns.pkl\")"
      ],
      "metadata": {
        "id": "kKZKGMbPUdcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load feature list in Flask\n",
        "feature_columns = joblib.load(\"/content/feature_columns.pkl\")"
      ],
      "metadata": {
        "id": "d5HdTshwUffH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from pyngrok import ngrok\n",
        "import time\n",
        "import joblib\n",
        "import pandas as pd\n",
        "\n",
        "# ========== INIT ==========\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Load model & feature list\n",
        "model = joblib.load(\"/content/predictive_maintenance_model.pkl\")\n",
        "\n",
        "print(\"Model & feature columns loaded\")\n",
        "\n",
        "# ========== ROUTE ==========\n",
        "@app.route(\"/predict\", methods=[\"POST\"]) #Creates an API URL & It accepts only POST requests(This means you send data to the API (not via browser link))\n",
        "def predict(): #This function runs whenever someone sends data to /predict\n",
        "    try:\n",
        "        start_time = time.time() #Records the current time\n",
        "\n",
        "        data = request.get_json() #Reads the incoming JSON data\n",
        "        if data is None:\n",
        "            return jsonify({\"error\": \"No JSON received\"}), 400\n",
        "\n",
        "        input_df = pd.DataFrame([data]) #Converts JSON into a pandas DataFrame\n",
        "\n",
        "        # Add missing features with default 0\n",
        "        for col in feature_columns:\n",
        "            if col not in input_df.columns:\n",
        "                input_df[col] = 0\n",
        "\n",
        "        # Keep only training features in correct order\n",
        "        input_df = input_df[feature_columns]\n",
        "\n",
        "        failure_prob = model.predict_proba(input_df)[0][1]\n",
        "\n",
        "        latency = (time.time() - start_time) * 1000 #Finds how long the API took to respond & Converts seconds â†’ milliseconds\n",
        "\n",
        "        return jsonify({\n",
        "            \"failure_probability\": round(float(failure_prob), 4),\n",
        "            \"latency_ms\": round(latency, 2)\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "\n",
        "# ========== RUN ==========\n",
        "if __name__ == \"__main__\":\n",
        "    public_url = ngrok.connect(5000)\n",
        "    print(\"Public URL:\", public_url)\n",
        "\n",
        "    app.run(host=\"0.0.0.0\", port=5000, debug=False)"
      ],
      "metadata": {
        "id": "AcnJBYEwQnXv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}